{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapasi1234/solar-forecasting-using-INSAT-3D-satelite-images/blob/main/GHI_Integrated_Code_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to execute\n",
        "  1. Order the hdf5 data from MOSDAC website. we have to order two folders one for calculating clear and cloudy pixel values and the other for calculating GHI values.\n",
        "  2. Mount the drive.\n",
        "  3. Provide the necessary details and run the \"Auto-Download the data\" cell.\n",
        "  4. Download the shape file of your area of interest and place it in the main directory.\n",
        "  5. Download one tif file from mosdac website and place it in the main directory.\n",
        "  6. place one sample file for AOD in the main directory.\n",
        "  7. Mention the AOI and Process and run the \"Initial Processing\" cell.\n",
        "  8. Initially, select the process as cloud detection,run the \"initialization\" cell and run the \"cloud detection process\" cell to calculate the clear and cloudy pixel for the particular Area\n",
        "  9. The use the clear and cloudy pixel values to calculate the Cloud Index.\n",
        "  10. After calculating cloud index, select the process as Forecasting,run the \"initialization\" cell. Inside initialization cell we need to provide the forecasting algorithm to be used.\n",
        "  11. we need to download the aerosol data file and perciptable water data file based on the test_data and run the \"GHI Calculation process\" cell.\n",
        "  12. In order the evaluate the Model performance, Get the ground data GHI values for that area and correlate with the normal and the forecasted GHI values and also check the error using Root Mean Squared Error metrics. To do that provide the necessary details in the \"Error metrics\" cell and run it.\n"
      ],
      "metadata": {
        "id": "GTD2sNYIRaX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "ek9DaieI3RJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auto-Download the data\n",
        "#@title Auto-Download the data { display-mode: \"form\" }\n",
        "Username = \"\" #@param {type:\"string\"}\n",
        "Password = \"\" #@param {type:\"string\"}\n",
        "Folder_name_for_data = \"\" #@param {type:\"string\"}\n",
        "Folder_name_for_test_data = \"\" #@param {type:\"string\"}\n",
        "cnopts=pysftp.CnOpts()\n",
        "cnopts.hostkeys=None\n",
        "os.makedirs('/content/drive/MyDrive/Project/Data')\n",
        "os.mkdir('/content/drive/MyDrive/Project/Test_Data')\n",
        "with pysftp.Connection('103.99.192.65', username=Username, password=Password,cnopts=cnopts,port=22) as sftp:\n",
        "  # initially, we need to place the order in the mosdac website and the folder name. Note the folder name and pass it as parameter.\n",
        "  #one folder for calculating clear and cloudy values and other for calculating the GHI values\n",
        "  # In the get_d method the first option is remote dir path and the second option is local dir path.\n",
        "  sftp.get_d('Order/'+Folder_name_for_data,'/content/drive/MyDrive/Project/Data')\n",
        "  sftp.get_d('Order/'+Folder_name_for_test_data,'/content/drive/MyDrive/Project/Test_Data')\n",
        "  sftp.close()"
      ],
      "metadata": {
        "id": "7X0_0S8zDadI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Installing necessary packages\n",
        "!pip install pysftp\n",
        "!pip install gdal\n",
        "!pip install fiona\n",
        "!pip install osgeo\n",
        "!pip install shapely\n",
        "!pip install rasterio\n",
        "!pip install geopandas\n",
        "!pip install rioxarray\n",
        "!pip install xarray\n",
        "!pip install earthpy\n",
        "!pip install pvlib\n",
        "!pip install blockmatching\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "##Importing the packages\n",
        "import os\n",
        "import shutil\n",
        "import h5py\n",
        "import glob\n",
        "import rasterio as rio\n",
        "import rasterio.plot\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from numpy import savetxt\n",
        "import pysftp\n",
        "import math\n",
        "import datetime\n",
        "from PIL import Image\n",
        "import geopandas as gd\n",
        "import rioxarray as rxr\n",
        "import xarray as xr\n",
        "from shapely.geometry import mapping, box\n",
        "import earthpy as et\n",
        "import earthpy.plot as ep\n",
        "import base64\n",
        "import numpy.ma as ma\n",
        "import pvlib\n",
        "from pvlib import *\n",
        "from pvlib.location import Location\n",
        "from blockmatching import *\n",
        "from datetime import timedelta\n",
        "from types import LambdaType\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "\n",
        "#initializations\n",
        "#@title Initial Processing { display-mode: \"form\" }\n",
        "AOI = \"pokhran\" #@param [\"pokhran\", \"tirupati\", \"kutch\", \"trichy\"]\n",
        "Process = \"Cloud Detection\" #@param [\"Cloud Detection\", \"Forecasting\"]\n",
        "\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "# Initial processing of data for cloud mask\n",
        "def initial_processing_cloud_mask(filepath,filename,dir,AOI):\n",
        "  f=h5py.File(filepath,\"r\")\n",
        "  vis=np.squeeze(np.array(f['IMG_VIS'][:]))\n",
        "  sun_elevation=np.array(f['Sun_Elevation'][:])\n",
        "  sun_elevation=np.squeeze(sun_elevation)/100\n",
        "  solar_zenith=np.sin(np.radians(sun_elevation))\n",
        "  vis_corrected=vis/solar_zenith\n",
        "  vis_rad=np.uint8(0.112135*vis_corrected-3.30421)\n",
        "  for x in np.nditer(vis_rad,op_flags=[\"readwrite\"]):\n",
        "    x[x<0]=0\n",
        "  sample_file=rio.open(dir+\"/sample_file.tif\")\n",
        "  desired_band=[0]\n",
        "  with rio.open('/content/tiff_files/vis/'+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_vis.tiff\",\"w\",driver=\"Gtiff\",height=sample_file.height,width=sample_file.width,count=1,dtype=vis_rad.dtype,crs=sample_file.crs,transform=sample_file.transform) as dst:\n",
        "    dst.write(vis_rad,indexes=1)\n",
        "  vis_t=rxr.open_rasterio('/content/tiff_files/vis/'+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_vis.tiff\",variable=desired_band).squeeze()\n",
        "  Area_of_Interest=gd.read_file(dir+\"/Shape_file/\"+AOI+\"/\"+AOI+\".shp\")\n",
        "  Area_of_Interest.geometry\n",
        "  crop_bound_box = [box(*Area_of_Interest.total_bounds)]\n",
        "  crop_bound_box[0]\n",
        "  Area_of_Interest.geometry[0]\n",
        "  vis_clipped = vis_t.rio.clip(crop_bound_box,crs=Area_of_Interest.crs,all_touched=False,from_disk=False).squeeze()\n",
        "  plt.imsave('/content/vis_img_'+AOI+'/'+filename.split('_')[1]+'_'+filename.split('_')[2]+\"_vis.png\",vis_rad)\n",
        "  return vis_clipped\n",
        "\n",
        "# calculating the cloud mask values based on the visible radiance data\n",
        "def cloud_mask(filepath,dir,imgpath,AOI):\n",
        "  f=h5py.File(filepath,\"r\")\n",
        "  filename=f.split('/')[8]\n",
        "  img=cv.imread(imgpath)\n",
        "  Z = img.reshape((-1,3))\n",
        "  Z = np.float32(Z)\n",
        "  criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "  K = 2\n",
        "  ret,label,center=cv.kmeans(Z,K,None,criteria,10,cv.ADAPTIVE_THRESH_MEAN_C)\n",
        "  center = np.uint8(center)\n",
        "  res = center[label.flatten()]\n",
        "  res2 = res.reshape((img.shape))\n",
        "  res2bw = cv.cvtColor(res2, cv.COLOR_BGR2GRAY)\n",
        "  colors = np.unique(res2bw)\n",
        "  colors = colors[1:]  \n",
        "  for i in range(0, len(colors)):\n",
        "    filter_mask = (res2bw == colors[i]).astype(int)\n",
        "  filter_mask=filter_mask.astype(\"int32\")\n",
        "  sample_file=rio.open(dir+\"/sample_file.tif\")\n",
        "  desired_band=[0]\n",
        "  with rio.open(\"/content/tiff_files/filter_mask/\"+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_filter_mask.tiff\",\"w\",driver=\"Gtiff\",height=file.height,width=file.width,count=1,dtype=filter_mask.dtype,crs=file.crs,transform=file.transform) as dst:\n",
        "    dst.write(filter_mask,indexes=1)\n",
        "  filter_mask_tiff=rxr.open_rasterio(\"/content/tiff_files/filter_mask/\"+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_filter_mask.tiff\",variable=desired_bands).squeeze()\n",
        "  Area_of_Interest=gd.read_file(dir+\"/Shape_file/\"+AOI+\"/\"+AOI+\".shp\")\n",
        "  Area_of_Interest.geometry\n",
        "  crop_bound_box = [box(*Area_of_Interest.total_bounds)]\n",
        "  crop_bound_box[0]\n",
        "  Area_of_Interest.geometry[0]\n",
        "  filter_mask_clip=filter_mask_tiff.rio.clip(crop_bound_box,crs=Area_of_Interest.crs,all_touched=False,from_disk=False).squeeze()  \n",
        "  filter_mask_clip=np.array(filter_mask_clip)\n",
        "  return filter_mask_clip\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "#To calculate the GHI\n",
        "def GHI_Calculation(filepath,filename,dir,AOI,dfname,ci):\n",
        "  f1=h5py.File(filepath)\n",
        "  starttime=f1.attrs['Acquisition_Start_Time'].decode('UTF-8')\n",
        "  endtime=f1.attrs['Acquisition_End_Time'].decode('UTF-8')\n",
        "  if AOI=='kutch':\n",
        "    a=0.04\n",
        "    b=0.96\n",
        "    altitude=74\n",
        "    lat=23.334\n",
        "    lon=70.637\n",
        "  elif AOI=='pokhran':\n",
        "    a=0.04\n",
        "    b=0.96\n",
        "    altitude=293\n",
        "    lat=26.916\n",
        "    lon=71.928\n",
        "  elif AOI=='tirupati':\n",
        "    a=0.03\n",
        "    b=0.97\n",
        "    altitude=194\n",
        "    lat=13.627\n",
        "    lon=79.397\n",
        "  elif AOI=='trichy':\n",
        "    a=0.03\n",
        "    b=0.97\n",
        "    altitude=87\n",
        "    lat=10.76\n",
        "    lon=78.813\n",
        "  k=a+b*(1-ci)\n",
        "  k=np.array(k)\n",
        "  location=Location(lat,lon,altitude=altitude,name=AOI)\n",
        "  AOD=AOD_calculation(filename,AOI,dir)\n",
        "  pwd=pwd_calculation(filename,AOI,dir,starttime)\n",
        "  clearsky=location.get_clearsky(pd.date_range(start=starttime, end=endtime,freq='1min'),model='simplified_solis',aod700=AOD,precipitable_water=pwd)\n",
        "  clear_ghi=np.array(clearsky[\"ghi\"])\n",
        "  clear_ghi_mean=np.array(clear_ghi.mean())\n",
        "  actual_ghi=clear_ghi_mean*k\n",
        "  date=pd.to_datetime(starttime)\n",
        "  date1=date+timedelta(hours=5,minutes=30)\n",
        "  dfname.loc[-1]=[date1,actual_ghi]\n",
        "  dfname.index=dfname.index+1\n",
        "  return dfname\n",
        "\n",
        "#To calculate Aerosol Optical Depth Value for solis method\n",
        "def AOD_calculation(filename,AOI,dir):\n",
        "  date=pd.to_datetime(filename.split('_')[1])\n",
        "  date_time=date.strftime(\"%Y%m%d\")\n",
        "  AOD_shp=gd.read_file(dir+\"/Shape_file/\"+AOI+\"_AOD/\"+AOI+\"_AOD.shp\")\n",
        "  sample_file=rio.open(dir+\"/sample_AOD_file.nc4\")\n",
        "  desired_bands=\"AODANA\"\n",
        "  aod4=rxr.open_rasterio(dir+\"/sample_AOD_file.nc4\",masked=True,variable=desired_bands)\n",
        "  aod4.rio.set_crs(\"epsg:4326\")\n",
        "  aod_crs=aod4.rio.crs\n",
        "  crs=aod_crs\n",
        "  for subdir1 in os.listdir(dir):\n",
        "    if subdir1==\"aerosol data\":\n",
        "      subdir1=os.path.join(dir,subdir1)\n",
        "      for subdir2 in os.listdir(subdir1): ## separate folders for each month\n",
        "        subdir2=os.path.join(subdir1,subdir2)\n",
        "        for filename1 in os.listdir(subdir2):\n",
        "          aod_file=os.path.join(subdir2,filename1)\n",
        "          if filename1.endswith(\".nc4\"):\n",
        "            aod_filename=filename1.split('.')[2]\n",
        "            if aod_filename==date_time:\n",
        "              aod=h5py.File(aod_file)\n",
        "              aod=aod.get(\"AODANA\")\n",
        "              for i in range(0,len(aod)):\n",
        "                if i==1 or i==2 or i==3:\n",
        "                  list=aod[i].shape\n",
        "                  with rio.open(\"/content/tiff_files/aod/aod_\"+filename.split('_')[1]+'_'+filename.split('_')[2]+\".tiff\",\"w\",driver=\"Gtiff\",height=list[0],width=list[1],count=1,dtype=aod.dtype,crs=crs,transform=sample_file.transform) as dst:\n",
        "                    dst.write(aod[i],indexes=1)\n",
        "                  aod_clip=rxr.open_rasterio(\"/content/tiff_files/aod/aod_\"+filename.split('_')[1]+'_'+filename.split('_')[2]+\".tiff\",variable=\"AODANA\",masked=True).squeeze()\n",
        "                  crop_bound_box = [box(*AOD_shp.total_bounds)]\n",
        "                  aod_clipped= aod_clip.rio.clip(crop_bound_box,crs=AOD_shp.crs,all_touched=False,from_disk=False).squeeze() \n",
        "                  stacked=np.array(aod_clipped).squeeze()\n",
        "                  aod_total=np.concatenate(stacked)\n",
        "  AOD=aod_total.mean()   \n",
        "  return AOD\n",
        "\n",
        "#To calculate the pwd value for solis method\n",
        "def pwd_calculation(filename,AOI,dir,starttime):\n",
        "  pwd_dir='pwd_'+starttime.split('-')[1]+' '+starttime.split('-')[2].split('T')[0]\n",
        "  AOD_shp=gd.read_file(dir+\"/Shape_file/\"+AOI+\"/\"+AOI+\".shp\")\n",
        "  for subdir1 in os.listdir(dir):\n",
        "    if subdir1==pwd_dir:\n",
        "      subdir1=os.path.join(dir,subdir1)\n",
        "      for filename2 in os.listdir(subdir1):\n",
        "        if filename2.endswith(\".nc4\"):\n",
        "          pwd_file=os.path.join(subdir1,filename2)\n",
        "          sample_file=rio.open(pwd_file)\n",
        "          desired_bands=\"TQV\"\n",
        "          pwd=rxr.open_rasterio(pwd_file,masked=True,variable=desired_bands)\n",
        "          pwd.rio.set_crs(\"epsg:4326\")\n",
        "          pwd_crs=pwd.rio.crs\n",
        "          crs=pwd_crs\n",
        "          f1=h5py.File(pwd_file)\n",
        "          TQV=np.array(f1.get(\"TQV\"))\n",
        "          with rio.open(\"/content/tiff_files/pwd/pwd_\"+filename.split('_')[1]+'_'+filename.split('_')[2]+\".tiff\",\"w\",driver=\"Gtiff\",height=sample_file.height,width=sample_file.width,count=1,dtype=TQV.dtype,crs=crs,transform=sample_file.transform) as dst:\n",
        "            dst.write(TQV)\n",
        "          pwd_clip=rxr.open_rasterio(\"/content/tiff_files/pwd/pwd_\"+filename.split('_')[1]+'_'+filename.split('_')[2]+\".tiff\",variable=\"AODANA\",masked=True).squeeze()\n",
        "          crop_bound_box = [box(*AOD_shp.total_bounds)]\n",
        "          pwd_clipped= pwd_clip.rio.clip(crop_bound_box,crs=AOD_shp.crs,all_touched=False,from_disk=False).squeeze() \n",
        "          pwd_mean=np.array(pwd_clipped).squeeze().mean()\n",
        "  return pwd_mean\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "\n",
        "# Initial processing of data for ghi calculation\n",
        "def initial_processing(filepath,filename,dir,AOI):\n",
        "  f=h5py.File(filepath,\"r\")\n",
        "  vis=np.squeeze(np.array(f['IMG_VIS'][:]))\n",
        "  sun_elevation=np.array(f['Sun_Elevation'][:])\n",
        "  sun_elevation=np.squeeze(sun_elevation)/100\n",
        "  solar_zenith=np.sin(np.radians(sun_elevation))\n",
        "  vis_corrected=vis/solar_zenith\n",
        "  vis_rad=np.uint8(0.112135*vis_corrected-3.30421)\n",
        "  for x in np.nditer(vis_rad,op_flags=[\"readwrite\"]):\n",
        "    x[x<0]=0\n",
        "  sample_file=rio.open(dir+\"/sample_file.tif\")\n",
        "  desired_band=[0]\n",
        "  with rio.open('/content/tiff_files/vis/'+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_vis.tiff\",\"w\",driver=\"Gtiff\",height=sample_file.height,width=sample_file.width,count=1,dtype=vis_rad.dtype,crs=sample_file.crs,transform=sample_file.transform) as dst:\n",
        "    dst.write(vis_rad,indexes=1)\n",
        "  vis_t=rxr.open_rasterio('/content/tiff_files/vis/'+filename.split('_')[1]+\"_\"+filename.split('_')[2]+\"_vis.tiff\",variable=desired_band).squeeze()\n",
        "  return vis_t\n",
        "\n",
        "# clip the visible data info of area of interest from given file\n",
        "def clip_AOI(vis,f,AOI,method):\n",
        "  Area_of_Interest=gd.read_file(dir+\"/Shape_file/\"+AOI+\"/\"+AOI+\".shp\")\n",
        "  Area_of_Interest.geometry\n",
        "  crop_bound_box = [box(*Area_of_Interest.total_bounds)]\n",
        "  crop_bound_box[0]\n",
        "  Area_of_Interest.geometry[0]\n",
        "  vis_clipped = vis.rio.clip(crop_bound_box,crs=Area_of_Interest.crs,all_touched=False,from_disk=False).squeeze()\n",
        "  return vis_clipped\n",
        "##-------------------------------------------------------------------------------------------------------------------------##\n",
        "#Forecasting Methods\n",
        "\n",
        "#Block Matching Method to forecast the GHI\n",
        "def BM(frame,old_frame):\n",
        "  background = BackgroundSubtractor(0.01, old_frame.astype(np.uint8), sgm = 1)\n",
        "  foreground = background.foreground(old_frame.astype(np.uint8))\n",
        "  frame0 = foreground.copy()\n",
        "  frame1 = background.foreground(frame.astype(np.uint8))\n",
        "  width = 4\n",
        "  height = 4\n",
        "  XP, YP, XD, YD = block_matching(frame0, frame1, width, height)\n",
        "  U, V, object_tops, meand = clustering(XD, YD, XP, YP, smooth=10, maxsizegraph=10)\n",
        "  lars = layers(frame, object_tops, width, height, sigma=10)\n",
        "  sigma = 10\n",
        "  r = zeros_like(frame)\n",
        "  for i in range(len(meand)):\n",
        "    c = 0.4\n",
        "    dsy, dsx = int(c * meand[i][0]), int(c * meand[i][1])\n",
        "    fill_width = (0, abs(dsx)) if dsx < 0 \\\n",
        "    else (abs(dsx), 0)\n",
        "    fill_height = (0, abs(dsy)) if dsy < 0 \\\n",
        "    else (abs(dsy), 0)\n",
        "    tmp = pad(lars[i], (fill_height, fill_width), mode='constant')\n",
        "    if dsy == 0 and dsx < 0:\n",
        "      tmp = tmp[:, abs(dsx):]\n",
        "    elif dsy == 0 and dsx > 0:\n",
        "      tmp = tmp[:, :-abs(dsx)]\n",
        "    elif dsy > 0 and dsx == 0:\n",
        "      tmp = tmp[:-abs(dsy), :]\n",
        "    elif dsy < 0 and dsx == 0:\n",
        "      tmp = tmp[abs(dsy):, :]\n",
        "    elif dsy < 0 and dsx < 0:\n",
        "      tmp = tmp[abs(dsy):, abs(dsx):]\n",
        "    elif dsy < 0 and dsx > 0:\n",
        "      tmp = tmp[abs(dsy):, :-abs(dsx)]\n",
        "    elif dsy > 0 and dsx < 0:\n",
        "      tmp = tmp[:-abs(dsy), abs(dsx):]\n",
        "    elif dsy > 0 and dsx > 0:\n",
        "      tmp = tmp[:-abs(dsy), :-abs(dsx)]\n",
        "    r = r + tmp\n",
        "    kernel = ones((sigma, sigma), float32) / sigma ** 2\n",
        "    forecast = cv.filter2D(r, -1, kernel)\n",
        "    t1 = forecast.copy()\n",
        "    t2 = background.background.copy()\n",
        "    ret, mask = cv.threshold(t1, 5, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    mask_inv = cv.bitwise_not(mask)\n",
        "    img1_bg = cv.bitwise_and(t1,t1,mask = mask)\n",
        "    img2_fg = cv.bitwise_and(t2,t2,mask = mask_inv)\n",
        "    dst = cv.add(img1_bg,img2_fg)\n",
        "    return dst\n",
        "\n",
        "#Optical Flow Method to forecast the GHI\n",
        "def optical_flow_Method(ci1,ci2):\n",
        "  optical_flow = cv.optflow.DualTVL1OpticalFlow_create(0.1,0.1,0.3,3,3,0.01,10,2,0.5,0.1)\n",
        "  flow=optical_flow.calc(ci1,ci2,None)\n",
        "  h, w = flow.shape[:2]\n",
        "  flow=flow\n",
        "  flow[:,:,0] += np.arange(w)\n",
        "  flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
        "  forecast = cv.remap(ci2, flow, None, cv.INTER_LINEAR)\n",
        "  forecast_ci=forecast.mean()\n",
        "  return forecast_ci\n"
      ],
      "metadata": {
        "id": "9WzIi9x3QEtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializations"
      ],
      "metadata": {
        "id": "hAAypnMQU2-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main Code Execution{ display-mode: \"code\" }\n",
        "dir='/content/drive/MyDrive/Project' ## please change the directory name\n",
        "\n",
        "# to remove the folder \n",
        "!rm -r '/content/tiff_files/'\n",
        "!rm -r '/content/vis_img_'+AOI+'/'\n",
        "\n",
        "if Process == \"Cloud Detection\":\n",
        "  os.makedirs('/content/tiff_files/vis/')\n",
        "  os.makedirs('/content/tiff_files/filter_mask/')\n",
        "  os.makedirs('/content/vis_img_'+AOI+'/')\n",
        "  os.mkdir(dir+'/clear and cloudy pixels/')\n",
        "  df_clear=pd.DataFrame(columns=['clear_pixels'])\n",
        "  df_cloudy=pd.DataFrame(columns=['cloudy_pixels'])\n",
        "elif Process == \"Forecasting\":\n",
        "  Forecast_Method = \"BM\"  # please Mention the method name as BM or optical_flow\n",
        "  os.makedirs('/content/tiff_files/vis/')\n",
        "  os.mkdir('/content/tiff_files/aod/')\n",
        "  os.mkdir('/content/tiff_files/pwd/')\n",
        "  df_ghi_normal=pd.DataFrame(columns=['Date','GHI_'+AOI])\n",
        "  df_ghi_forecast=pd.DataFrame(columns=['Date','Forecast_GHI_'+AOI])"
      ],
      "metadata": {
        "id": "NyEgA4D7bO7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloud Detection Process"
      ],
      "metadata": {
        "id": "fonB8zX6U7e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for clear and cloudy pixel calculations\n",
        "for subdir1 in os.listdir(dir):\n",
        "  if subdir1==\"Data\": ## please change the folder name for calculatingg the clear n cloudy pixels\n",
        "    subdir1=os.path.join(dir,subdir1) \n",
        "    for filename in os.listdir(subdir1):\n",
        "      file=os.path.join(subdir1,filename)\n",
        "      if(filename.endswith(\".h5\")):\n",
        "        vis_clipped=initial_processing_cloud_mask(file,filename,dir,AOI)\n",
        "        filter_mask_clipped=cloud_mask(file,dir,dir+'/content/vis_img_'+AOI+'/'+filename.split('_')[1]+'_'+filename.split('_')[2]+\"_vis.png\",AOI)\n",
        "        filter_mask_clipped_clear=(filter_mask_clipped==0)\n",
        "        filter_mask_clipped_cloudy=(filter_mask_clipped==1)\n",
        "        vis_clipped_np=np.array(vis_clipped)\n",
        "        clear_pixels=vis_clipped_np[filter_mask_clipped_clear]\n",
        "        if clear_pixels.size!=0:\n",
        "          for x in clear_pixels:\n",
        "            df_clear.loc[-1]=x\n",
        "            df_clear.index=df_clear.index+1\n",
        "        cloudy_pixels=vis_clipped_np[filter_mask_clipped_cloudy]\n",
        "        if cloudy_pixels.size!=0:\n",
        "          for x in cloudy_pixels:\n",
        "            df_cloudy.loc[-1]=x\n",
        "            df_cloudy.index=df_cloudy.index+1\n",
        "  df_clear.sort_index().to_csv(dir+'/clear and cloudy pixels/'+AOI+'_clear.csv')\n",
        "  df_cloudy.sort_index().to_csv(dir+'/clear and cloudy pixels/'+AOI+'_cloudy.csv')"
      ],
      "metadata": {
        "id": "BgoIRk-to7ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GHI Calculation Process"
      ],
      "metadata": {
        "id": "gTsf9WNXVBjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for GHI calculations\n",
        "for subdir1 in os.listdir(dir):\n",
        "    if subdir1==\"Test_Data\": ## please change the folder name for calculating the ghi values\n",
        "      subdir1=os.path.join(dir,subdir1) \n",
        "      for subdir2 in os.listdir(subdir1): ## create separate folders for each day from the given data\n",
        "        subdir2=os.path.join(subdir1,subdir2)\n",
        "        filepaths  = sorted([os.path.join(subdir2, filename) for filename in os.listdir(subdir2)])\n",
        "        for x in range(0,(len(filepaths)-2)):\n",
        "          f1=h5py.File(filepaths[x],'r')\n",
        "          filename1=filepaths[x].split('/')[8]\n",
        "          vis1=initial_processing(filepaths[x],filename1,dir,AOI)\n",
        "          vis_clipped1=clip_AOI(vis1,dir,AOI,Forecast_Method)\n",
        "          clear=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_clear.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "          cloudy=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_cloudy.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "          minimum=np.percentile(clear,5)\n",
        "          maximum=np.percentile(cloudy,98) \n",
        "          ci1=(vis_clipped1-minimum)/(maximum-minimum)\n",
        "          ci_1=ci1.mean()\n",
        "          if ci_1<0:\n",
        "            ci_1=0\n",
        "          elif ci_1>1:\n",
        "            ci_1=1\n",
        "          df_ghi_normal=GHI_Calculation(filepaths[x],filename1,dir,AOI,df_ghi_normal,ci_1)\n",
        "          f2=h5py.File(filepaths[x+1],'r')\n",
        "          filename2=filepaths[x+1].split('/')[8]\n",
        "          vis2=initial_processing(filepaths[x+1],filename2,dir,AOI)\n",
        "          vis_clipped2=clip_AOI(vis2,dir,AOI,Forecast_Method)\n",
        "          clear=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_clear.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "          cloudy=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_cloudy.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "          minimum=np.percentile(clear,5)\n",
        "          maximum=np.percentile(cloudy,98)\n",
        "          ci2=(vis_clipped2-minimum)/(maximum-minimum)\n",
        "          ci_2=ci2.mean()\n",
        "          if ci_2<0:\n",
        "            ci_2=0\n",
        "          elif ci_2>1:\n",
        "            ci_2=1\n",
        "          df_ghi_normal=GHI_Calculation(filepaths[x+1],filename2,dir,AOI,df_ghi_normal,ci_2)\n",
        "          filename3=filepaths[x+2].split('/')[8]\n",
        "          c_f1=np.uint8(vis_clipped1)\n",
        "          c_f2=np.uint8(vis_clipped2)\n",
        "          if Forecast_Method=='BM':\n",
        "            vis_forecast=BM(c_f2,c_f1) \n",
        "            if(vis_forecast is np.ndarray):\n",
        "              sample_file=rio.open(dir+\"/3DIMG_01JUL2021_0400_L1C_ASIA_MER_IMG_VIS.tif\")\n",
        "              desired_band=[0]\n",
        "              with rio.open(\"/content/tiff_files/vis/\"+filename3.split('_')[1]+'_'+filename3.split('_')[2]+\"_vis.tiff\",\"w\",driver=\"Gtiff\",height=sample_file.height,width=sample_file.width,count=1,dtype=vis_forecast.dtype,crs=sample_file.crs,transform=sample_file.transform) as dst:\n",
        "                dst.write(vis_forecast,indexes=1)\n",
        "              vis_t=rxr.open_rasterio(\"/content/tiff_files/vis/\"+filename3.split('_')[1]+'_'+filename3.split('_')[2]+\"_vis.tiff\",variable=desired_band).squeeze()\n",
        "              vis_forecast_clipped=vis_t\n",
        "              clear=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_clear.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "              cloudy=np.genfromtxt(dir+'/clear and cloudy pixels/'+AOI+'_cloudy.csv',dtype=int,delimiter=\",\",skip_header=1,usecols=1)\n",
        "              minimum=np.percentile(clear,5)\n",
        "              maximum=np.percentile(cloudy,98) \n",
        "              ci_forecast=(vis_forecast_clipped-minimum)/(maximum-minimum)\n",
        "              ci_forecast_mean=ci_forecast.mean()\n",
        "          elif Forecast_Method=='optical_flow':\n",
        "            ci_forecast_mean=optical_flow_Method(ci1,ci2)\n",
        "          if ci_forecast_mean<0:\n",
        "            ci_forecast_mean=0\n",
        "          elif ci_forecast_mean>1:\n",
        "            ci_forecast_mean=1\n",
        "          df_ghi_forecast=GHI_Calculation(filepaths[x+2],filename3,dir,AOI,df_ghi_forecast,ci_forecast_mean)\n",
        "\n",
        "    df_ghi_normal.drop_duplicates(subset = [\"Date\"]).sort_values('Date').to_csv(dir+'/'+Forecast_Method+'_ghi_normal_'+AOI+'.csv')\n",
        "    df_ghi_forecast.drop_duplicates(subset = [\"Date\"]).sort_values('Date').to_csv(dir+'/'+Forecast_Method+'_ghi_forecast_'+AOI+'.csv')\n"
      ],
      "metadata": {
        "id": "Dq5SpHcgoNyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error metrics"
      ],
      "metadata": {
        "id": "yyJLSeGqtZHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Normal_GHI_calculated=pd.read_csv(dir+'/'+Forecast_Method+'_ghi_normal_'+AOI+'.csv')\n",
        "Forecast_GHI_calculated=pd.read_csv(dir+'/'+Forecast_Method+'_ghi_forecast_'+AOI+'.csv')\n",
        "GHI_ground=pd.read_csv(dir+'/'+AOI+'_ground_data.csv')\n",
        "n_col1=Normal_GHI_calculated['GHI_'+AOI]\n",
        "f_col1=Forecast_GHI_calculated['Forecast_GHI_'+AOI]\n",
        "col2=GHI_ground['GHI_Ground_Data']\n",
        "normal_ghi_cor=n_col1.corr(col2)\n",
        "print(normal_ghi_cor)\n",
        "forecast_ghi_cor=n_col1.corr(col2)\n",
        "print(forecast_ghi_cor)\n",
        "n_val1=n_col1.values\n",
        "f_val1=f_col1.values\n",
        "val2=col2.values\n",
        "normal_ghi_rmse=math.sqrt(mean_squared_error(n_val1, val2))\n",
        "print(normal_ghi_rmse)\n",
        "forecast_ghi_rmse=math.sqrt(mean_squared_error(f_val1, val2))\n",
        "print(forecast_ghi_rmse)"
      ],
      "metadata": {
        "id": "b21fF0RlELPQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hAAypnMQU2-z",
        "fonB8zX6U7e1",
        "gTsf9WNXVBjV",
        "yyJLSeGqtZHX"
      ],
      "name": "GHI_Integrated_Code_Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}